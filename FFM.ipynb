{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Documentation: <br>\n",
    "https://www.yelp.com/dataset/documentation/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import requests\n",
    "import xlearn as xl\n",
    "import networkx as nx\n",
    "pd.options.mode.chained_assignment = None\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192609/192609 [00:02<00:00, 64243.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#line_count = len(open(\"data/review.json\").readlines())\n",
    "line_count = len(open(\"data/business.json\", encoding=\"utf8\").readlines())\n",
    "business_ids, cities, states, latitudes, longitudes, stars, review_counts = [], [], [], [], [], [], []\n",
    "with open(\"data/business.json\", encoding=\"utf8\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        cities += [blob[\"city\"]]\n",
    "        states += [blob[\"state\"]]\n",
    "        latitudes += [blob[\"latitude\"]]\n",
    "        longitudes += [blob[\"longitude\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        review_counts += [blob[\"review_count\"]]\n",
    "        \n",
    "businesses = pd.DataFrame(\n",
    "    {\"business_id\": business_ids, \"city\": cities, \"state\": states, \"latitude\": latitudes, \"longitude\": longitudes, \"business_average_stars\": stars, \"business_review_counts\": review_counts }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637138/1637138 [00:26<00:00, 62442.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#line_count = len(open(\"data/review.json\").readlines())\n",
    "line_count = len(open(\"data/user.json\", encoding=\"utf8\").readlines())\n",
    "users, review_counts, elites, average_stars, friends = [], [], [], [], []\n",
    "with open(\"data/user.json\", encoding=\"utf8\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        users += [blob[\"user_id\"]]\n",
    "        review_counts += [blob[\"review_count\"]]\n",
    "        elites += [blob[\"elite\"]]\n",
    "        average_stars += [blob[\"average_stars\"]]\n",
    "        friends += [blob[\"friends\"]]\n",
    "        \n",
    "users = pd.DataFrame(\n",
    "    {\"user_id\": users, \"user_review_counts\": review_counts,\"elite\": elites, \"user_average_stars\": average_stars, \"friends\": friends}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_holdout = pd.read_csv('data/ratings_sample_holdout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = pd.read_csv('data/ratings_sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_val = pd.read_csv('data/ratings_sample_cv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df = df.drop(df.columns[0], axis =1)\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users, on = 'user_id')\n",
    "    df = df.merge(businesses, on = 'business_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = process(ratings_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_holdout = process(ratings_holdout.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_val = process(ratings_val.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'business_id', 'rating', 'date', 'text', 'week_day', 'month',\n",
       "       'hour', 'user_review_counts', 'elite', 'user_average_stars', 'friends',\n",
       "       'city', 'state', 'latitude', 'longitude', 'business_average_stars',\n",
       "       'business_review_counts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 406042 rows, 18 columns in training set.\n",
      "There are 28612 rows, 18 columns in holdout set.\n",
      "There are 28615 rows, 18 columns in validation set.\n"
     ]
    }
   ],
   "source": [
    "print('There are {0} rows, {1} columns in training set.'.format(ratings_train.shape[0], ratings_train.shape[1]))\n",
    "print('There are {0} rows, {1} columns in holdout set.'.format(ratings_holdout.shape[0], ratings_holdout.shape[1]))\n",
    "print('There are {0} rows, {1} columns in validation set.'.format(ratings_val.shape[0], ratings_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def _convert_to_ffm(path, df, type, target, numerics, categories, features, encoder):\n",
    "    # Flagging categorical and numerical fields\n",
    "    print('convert_to_ffm - START')\n",
    "    for x in numerics:\n",
    "        if(x not in encoder['catdict']):\n",
    "            print(f'UPDATING CATDICT: numeric field - {x}')\n",
    "            encoder['catdict'][x] = 0\n",
    "    for x in categories:\n",
    "        if(x not in encoder['catdict']):\n",
    "            print(f'UPDATING CATDICT: categorical field - {x}')\n",
    "            encoder['catdict'][x] = 1\n",
    "\n",
    "    nrows = df.shape[0]\n",
    "    with open(path + str(type) + \"_ffm.txt\", \"w\") as text_file:\n",
    "\n",
    "        # Looping over rows to convert each row to libffm format\n",
    "        for n, r in enumerate(range(nrows)):\n",
    "            datastring = \"\"\n",
    "            datarow = df.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow[target]))  # Set Target Variable here\n",
    "\n",
    "            # For numerical fields, we are creating a dummy field here\n",
    "            for i, x in enumerate(encoder['catdict'].keys()):\n",
    "                if(encoder['catdict'][x] == 0):\n",
    "                    # Not adding numerical values that are nan\n",
    "                    if math.isnan(datarow[x]) is not True:\n",
    "                        datastring = datastring + \" \"+str(i)+\":\" + str(i)+\":\" + str(datarow[x])\n",
    "                else:\n",
    "\n",
    "                    # For a new field appearing in a training example\n",
    "                    if(x not in encoder['catcodes']):\n",
    "                        print(f'UPDATING CATCODES: categorical field - {x}')\n",
    "                        encoder['catcodes'][x] = {}\n",
    "                        encoder['currentcode'] += 1\n",
    "                        print(f'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}')\n",
    "                        encoder['catcodes'][x][datarow[x]] = encoder['currentcode']  # encoding the feature\n",
    "\n",
    "                    # For already encoded fields\n",
    "                    elif(datarow[x] not in encoder['catcodes'][x]):\n",
    "                        encoder['currentcode'] += 1\n",
    "                        print(f'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}')\n",
    "                        encoder['catcodes'][x][datarow[x]] = encoder['currentcode']  # encoding the feature\n",
    "\n",
    "                    code = encoder['catcodes'][x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\" + str(int(code))+\":1\"\n",
    "\n",
    "            datastring += '\\n'\n",
    "            text_file.write(datastring)\n",
    "\n",
    "    # print('Encoder Summary:')\n",
    "    # print(json.dumps(encoder, indent=4))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user text embeddings based on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_texts = ratings_train.groupby(['user_id'])['text'].apply(lambda x: ','.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_texts['text'] = rating_texts['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_texts['text'] = rating_texts['text'].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents='ascii',\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words = 'english', strip_accents = 'ascii')\n",
    "vectorizer.fit(rating_texts['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform(rating_texts['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv = TruncatedSVD(n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv.fit(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_tsv = tsv.transform(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwcss=[]\\nfor i in range(77, 100):\\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\\n    kmeans.fit(transformed_tsv)\\n    wcss.append(kmeans.inertia_)\\nplt.plot(range(2, 100), wcss)\\nplt.title('Elbow Method')\\nplt.xlabel('Number of clusters')\\nplt.ylabel('WCSS')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "wcss=[]\n",
    "for i in range(77, 100):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(transformed_tsv)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 100), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=110, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=110, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(transformed_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cluster = kmeans.predict(transformed_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_texts.loc[:,'text_cluster'] = text_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_texts_features = rating_texts[['user_id','text_cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train= ratings_train.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_holdout = ratings_holdout.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_val = ratings_val.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = ratings_train[['user_id','friends']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_dict = train_users.set_index('user_id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "g.add_nodes_from(train_users_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in train_users_dict.items():\n",
    "    for i in v[0].split(','):\n",
    "        g.add_edge(k,i.strip())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graphs = nx.connected_component_subgraphs(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgs =[]\n",
    "for i, sg in enumerate(sub_graphs):\n",
    "    sgs += [sg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_sgs = []\n",
    "for i in sgs:\n",
    "    if len(i.nodes()) >=5:\n",
    "        fin_sgs +=[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_user_ids, graph_ids = [],[]\n",
    "num_id = 0\n",
    "for graph in fin_sgs:\n",
    "    for node in graph.nodes():\n",
    "        graph_user_ids += [node]\n",
    "        graph_ids += [num_id]\n",
    "    num_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_graphs = pd.DataFrame(\n",
    "    {\"user_id\": graph_user_ids, \"graph_cluster\": graph_ids}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train= ratings_train.merge(social_graphs, on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_holdout = ratings_holdout.merge(social_graphs, on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_val = ratings_val.merge(social_graphs, on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Location Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ratings_train[['longitude','latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nEXPLORATORY ANALYSIS TO DETERMINE NUMBER OF CLUSTERS. DON'T RUN\\nHERE USING THE EBLOW METHOD WE CHOOSE NCLUSTERs OF 10 \\n\\nX = ratings_train[['longitude','latitude']]\\nwcss = []\\nfor i in range(2, 100):\\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\\n    kmeans.fit(X)\\n    wcss.append(kmeans.inertia_)\\nplt.plot(range(2, 50), wcss[0:48])\\nplt.title('Elbow Method')\\nplt.xlabel('Number of clusters')\\nplt.ylabel('WCSS')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "EXPLORATORY ANALYSIS TO DETERMINE NUMBER OF CLUSTERS. DON'T RUN\n",
    "HERE USING THE EBLOW METHOD WE CHOOSE NCLUSTERs OF 10 \n",
    "\n",
    "X = ratings_train[['longitude','latitude']]\n",
    "wcss = []\n",
    "for i in range(2, 100):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 50), wcss[0:48])\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=10, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.loc[:,'location_cluster'] = kmeans.predict(ratings_train[['longitude','latitude']])\n",
    "ratings_holdout.loc[:,'location_cluster'] = kmeans.predict(ratings_holdout[['longitude','latitude']])\n",
    "ratings_val.loc[:,'location_cluster'] = kmeans.predict(ratings_val[['longitude','latitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.text_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.text_cluster.fillna('999', inplace=True)\n",
    "ratings_val.text_cluster.fillna('999', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.location_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.location_cluster.fillna('999', inplace=True)\n",
    "ratings_val.location_cluster.fillna('999', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.graph_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.graph_cluster.fillna('999', inplace=True)\n",
    "ratings_val.graph_cluster.fillna('999', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFM CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMModel:\n",
    "\n",
    "    def __init__(self, train, test, config, suffix = None):\n",
    "        self.train_df = train\n",
    "        self.test_df = test\n",
    "        self.model = xl.create_ffm()\n",
    "        self.suffix = suffix\n",
    "        self.config = config\n",
    "        self.preds = None \n",
    "          \n",
    "    def __configure(self):\n",
    "        destination = self.config['destination']\n",
    "        label = self.config['label']\n",
    "        numerical_columns  = self.config['numerical_columns']\n",
    "        categorical_columns  = self.config['categorical_columns']\n",
    "        all_columns  = numerical_columns + categorical_columns\n",
    "              \n",
    "        encoder = {\"currentcode\": len(self.config['numerical_columns']),\n",
    "           \"catdict\": {},\n",
    "           \"catcodes\": {}}\n",
    "        \n",
    "        encoder = _convert_to_ffm(destination, self.train_df , 'train', label, numerical_columns, categorical_columns, all_columns, encoder)\n",
    "        encoder = _convert_to_ffm(destination, self.test_df , 'test', label, numerical_columns, categorical_columns, all_columns, encoder)\n",
    "        \n",
    "    def train(self, params = None):\n",
    "        encoder = self.__configure()\n",
    "        self.model.setTrain(self.config['destination']+'train_ffm.txt')\n",
    "        self.model.setValidate(self.config['destination']+'test_ffm.txt')\n",
    "        self.model.setTest(self.config['destination']+'test_ffm.txt')\n",
    "        \n",
    "        if not params:\n",
    "            params = {'task': 'reg',\n",
    "                     'lr': 0.2,\n",
    "                     'lambda': 0.002,\n",
    "                     'metric': 'auc'}\n",
    "        \n",
    "        self.model.fit(params, self.config['model_destination']+self.config['model_name']+'.out')\n",
    "\n",
    "    def evaluate_on_val(self):\n",
    "        self.model.predict(self.config['model_destination'] + self.config['model_name']+'.out', self.config['output_destination']+'predictions.txt')\n",
    "        preds = pd.read_csv(self.config['output_destination']+'predictions.txt', sep=\" \", header=None)\n",
    "        preds.columns = ['prediction']\n",
    "        preds.prediction = np.clip(preds.prediction,0.0,5.0)\n",
    "        self.preds = preds\n",
    "        return preds            \n",
    "    \n",
    "    def get_regression_metrics(self):\n",
    "        if self.preds is None:\n",
    "            self.evaluate_on_val()\n",
    "        \n",
    "        predictions = self.evaluate_on_val()\n",
    "        test_df = self.test_df.copy()\n",
    "        test_df['preds']  = np.clip(predictions.values,0.0,5.0)\n",
    "        \n",
    "        r2 = r2_score(test_df['rating'], test_df['preds'])\n",
    "        mse = mean_squared_error(test_df['rating'], test_df['preds'])\n",
    "        mae = mean_absolute_error(test_df['rating'], test_df['preds'])\n",
    "        \n",
    "        print('R2: ', r2, ' MSE: ',mse, ' MAE: ', mae)\n",
    "        return r2,mse,mae\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics with Just userid and business id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'baseline',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_FFM = FFMModel(ratings_train, ratings_val, baseline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "baseline_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.16359044241141085  MSE:  1.8428614957708407  MAE:  1.115170516826839\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = baseline_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics with Baseline + Business Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_business_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'user_business',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_business_FFM = FFMModel(ratings_train, ratings_val, user_business_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "user_business_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.16561123600483585  MSE:  1.8384090804792699  MAE:  1.096584375956666\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = user_business_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics with Baseline + Location Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','location_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'location',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_FFM = FFMModel(ratings_train, ratings_val, location_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "location_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.16952274567049086  MSE:  1.8297908497479602  MAE:  1.0989993160580116\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = location_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics with User and Business Information + Text Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','text_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'text',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "text_FFM = FFMModel(ratings_train, ratings_val, text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "text_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.15673734312059995  MSE:  1.8579609320400061  MAE:  1.1026677958063953\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = text_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics with User and Business Information + Graph Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','graph_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'graph',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "graph_FFM = FFMModel(ratings_train, ratings_val, graph_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "graph_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.1665249930806647  MSE:  1.836395799167196  MAE:  1.0960562833129477\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = graph_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics All clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','graph_cluster','location_cluster','text_cluster'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'all_clusters',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "all_FFM = FFMModel(ratings_train, ratings_val, all_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "all_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.15706300571220033  MSE:  1.857243399528298  MAE:  1.0948153850008737\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = all_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFM runs in linear time which leads to fast runtime. Our model takes just two minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import time\n",
    "start_time = time.time()\n",
    "baseline_FFM = FFMModel(ratings_train, ratings_val, baseline_config)\n",
    "baseline_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 79.6070818901062 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serendipity and Ranking Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_city_businesses = val[['city','business_id']].drop_duplicates()\n",
    "unique_cities = unique_city_businesses.groupby('city').count()['business_id']\n",
    "unique_cities = unique_cities[unique_cities > 25]\n",
    "out = pd.DataFrame()\n",
    "for city in unique_cities.index:\n",
    "    tmp = val[(val['city'] ==city) & (val['rating'] >val['user_average_stars'])]\n",
    "    if len(tmp)>0:\n",
    "        row = tmp.sample(1)\n",
    "        out = out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = out[['user_id','city','state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = predict_df.merge(unique_city_businesses, on = 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dummy assingment to make it work with a dumb api \n",
    "predict_df['rating'] = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'baseline',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_FFM = FFMModel(ratings_train, predict_df, baseline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "baseline_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = baseline_FFM.evaluate_on_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df['predictions'] = predictions['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_recs = predict_df.groupby('user_id')['predictions'].nlargest(10).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt =0\n",
    "serendipity = 0\n",
    "for row in out.iterrows():\n",
    "    row_values = row[1]\n",
    "    top_10 = predict_df.loc[top_10_recs[top_10_recs['user_id'] == row_values['user_id']].level_1]['business_id']\n",
    "    ###In top 10\n",
    "    if row_values['business_id'] in top_10.values:\n",
    "        cnt+=1\n",
    "    user_history = ratings_train[ratings_train['user_id'] == row_values['user_id']]    \n",
    "    been_there = [i for i in top_10.values if i in  user_history.business_id.values]\n",
    "    serendipity += 1-len(been_there)/10\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.970270270270271"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###these are resturants in our top 10 recommendation that the user hasn't been \n",
    "serendipity/len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20270270270270271"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Visited restaurant is in top 10 recommendation. Ranking\n",
    "cnt/len(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
