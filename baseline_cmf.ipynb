{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Documentation: <br>\n",
    "https://www.yelp.com/dataset/documentation/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cmfrec import CMF\n",
    "import pycmf\n",
    "\n",
    "import time\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192609/192609 [00:02<00:00, 77157.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# load business.json\n",
    "# 192609 unique businesses?\n",
    "line_count = len(open(\"./yelp_dataset/business.json\").readlines())\n",
    "business_ids, cities, states, latitudes, longitudes, stars, review_counts, attributes, categories = [], [], [], [], [], [], [], [], []\n",
    "with open(\"./yelp_dataset/business.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        cities += [blob[\"city\"]]\n",
    "        states += [blob[\"state\"]]\n",
    "        latitudes += [blob[\"latitude\"]]\n",
    "        longitudes += [blob[\"longitude\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        review_counts += [blob[\"review_count\"]]\n",
    "        attributes += [blob[\"attributes\"]]\n",
    "        categories += [blob[\"categories\"]]\n",
    "        \n",
    "businesses = pd.DataFrame(\n",
    "    {\"business_id\": business_ids, \"city\": cities, \"state\": states, \"latitude\": latitudes, \"longitude\": longitudes, \"stars\": stars, \"review_counts\": review_counts, \"attributes\": attributes, \"categories\":categories }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637138/1637138 [00:20<00:00, 81318.74it/s] \n"
     ]
    }
   ],
   "source": [
    "# load user.json\n",
    "# 1637138 unique users?\n",
    "line_count = len(open(\"./yelp_dataset/user.json\").readlines())\n",
    "users, review_counts, elites, average_stars, friends = [], [], [], [], []\n",
    "with open(\"./yelp_dataset/user.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        users += [blob[\"user_id\"]]\n",
    "        review_counts += [blob[\"review_count\"]]\n",
    "        elites += [blob[\"elite\"]]\n",
    "        average_stars += [blob[\"average_stars\"]]\n",
    "        friends += [blob[\"friends\"]]\n",
    "        \n",
    "users = pd.DataFrame(\n",
    "    {\"user_id\": users, \"review_count\": review_counts,\"elite\": elites, \"average_stars\": average_stars, \"friends\": friends}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6685900/6685900 [00:57<00:00, 115327.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# load review.json\n",
    "# 6685900 unique reviews?\n",
    "line_count = len(open(\"./yelp_dataset/review.json\").readlines())\n",
    "user_ids, business_ids, stars, dates, texts = [], [], [], [], []\n",
    "with open(\"./yelp_dataset/review.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        user_ids += [blob[\"user_id\"]]\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        dates += [blob[\"date\"]]\n",
    "        texts += [blob[\"text\"]]\n",
    "reviews = pd.DataFrame(\n",
    "    {\"user_id\": user_ids, \"business_id\": business_ids, \"rating\": stars, \"date\": dates, \"text\": texts}\n",
    ")\n",
    "user_counts = reviews[\"user_id\"].value_counts()\n",
    "active_users = user_counts.loc[user_counts >= 5].index.tolist()\n",
    "reviews = reviews.loc[reviews.user_id.isin(active_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df = df.drop(df.columns[0], axis =1)\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users, on = 'user_id')\n",
    "    df = df.merge(businesses, on = 'business_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data: 20%, 50%, 100%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(803897, 20)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train_20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1997023, 20)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1208638, 20)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train_100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bowenzhou/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ratings_holdout_20 = pd.read_csv('data/ratings_sample_holdout_20.csv')\n",
    "ratings_train_20 = pd.read_csv('data/ratings_sample_train_20.csv')\n",
    "ratings_val_20 = pd.read_csv('data/ratings_sample_cv_20.csv')\n",
    "\n",
    "ratings_holdout_50 = pd.read_csv('data/ratings_sample_holdout_50.csv')\n",
    "ratings_val_50 = pd.read_csv('data/ratings_sample_cv_50.csv')\n",
    "ratings_train_50 = pd.read_csv('data/ratings_sample_train_50.csv')\n",
    "\n",
    "ratings_holdout_100 = pd.read_csv('data/ratings_sample_holdout_100.csv')\n",
    "ratings_train_100 = pd.read_csv('data/ratings_sample_train_100.csv')\n",
    "ratings_val_100 = pd.read_csv('data/ratings_sample_cv_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train_20 = process(ratings_train_20.copy())\n",
    "ratings_holdout_20 = process(ratings_holdout_20.copy())\n",
    "ratings_val_20 = process(ratings_val_20.copy())\n",
    "\n",
    "ratings_train_50 = process(ratings_train_50.copy())\n",
    "ratings_holdout_50 = process(ratings_holdout_50.copy())\n",
    "ratings_val_50 = process(ratings_val_50.copy())\n",
    "\n",
    "ratings_val_100 = process(ratings_val_100.copy())\n",
    "ratings_train_100 = process(ratings_train_100.copy())\n",
    "ratings_holdout_100 = process(ratings_holdout_100.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test_20 = ratings_holdout_20.loc[ratings_holdout_20.business_id.isin(ratings_train_20.business_id)]\n",
    "ratings_val_20 = ratings_val_20.loc[ratings_val_20.business_id.isin(ratings_train_20.business_id)]\n",
    "\n",
    "ratings_test_50 = ratings_holdout_50.loc[ratings_holdout_50.business_id.isin(ratings_train_50.business_id)]\n",
    "ratings_val_50 = ratings_val_50.loc[ratings_val_50.business_id.isin(ratings_train_50.business_id)]\n",
    "\n",
    "ratings_test_100 = ratings_holdout_100.loc[ratings_holdout_100.business_id.isin(ratings_train_100.business_id)]\n",
    "ratings_val_100 = ratings_val_100.loc[ratings_val_100.business_id.isin(ratings_train_100.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_20 = ratings_train_20.iloc[:,0:3]\n",
    "trainset_20.columns = ['userID', 'itemID','rating']\n",
    "valset_20 = ratings_val_20.iloc[:, 0:3]\n",
    "valset_20.columns = ['userID', 'itemID','rating']\n",
    "testset_20 = ratings_holdout_20.iloc[:, 0:3]\n",
    "testset_20.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "trainset_50 = ratings_train_50.iloc[:,0:3]\n",
    "trainset_50.columns = ['userID', 'itemID','rating']\n",
    "valset_50 = ratings_val_50.iloc[:, 0:3]\n",
    "valset_50.columns = ['userID', 'itemID','rating']\n",
    "testset_50 = ratings_holdout_50.iloc[:, 0:3]\n",
    "testset_50.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "trainset_100 = ratings_train_100.iloc[:,0:3]\n",
    "trainset_100.columns = ['userID', 'itemID','rating']\n",
    "valset_100 = ratings_val_100.iloc[:, 0:3]\n",
    "valset_100.columns = ['userID', 'itemID','rating']\n",
    "testset_100 = ratings_holdout_100.iloc[:, 0:3]\n",
    "testset_100.columns = ['userID', 'itemID','rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: SVD as Matrix Factorization\n",
    "\n",
    "As a CF algorithm. A matrix factorization technique that reduces the number of features of a data set by reducing space dimensions from N to K where K < N. Thus, in our context, we are finding 2 matrices whose product is the original matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(803897, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1997023, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3965887, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to work with surprise, 20%, 50% and full dataset\n",
    "reader = Reader(rating_scale = (0.0, 5.0))\n",
    "train_data_20 = Dataset.load_from_df(trainset_20[['userID','itemID','rating']], reader)\n",
    "val_data_20 = Dataset.load_from_df(valset_20[['userID','itemID','rating']], reader)\n",
    "test_data_20 = Dataset.load_from_df(testset_20[['userID','itemID','rating']], reader)\n",
    "\n",
    "train_data_50 = Dataset.load_from_df(trainset_50[['userID','itemID','rating']], reader)\n",
    "val_data_50 = Dataset.load_from_df(valset_50[['userID','itemID','rating']], reader)\n",
    "test_data_50 = Dataset.load_from_df(testset_50[['userID','itemID','rating']], reader)\n",
    "\n",
    "train_data_100 = Dataset.load_from_df(trainset_100[['userID','itemID','rating']], reader)\n",
    "val_data_100 = Dataset.load_from_df(valset_100[['userID','itemID','rating']], reader)\n",
    "test_data_100 = Dataset.load_from_df(testset_100[['userID','itemID','rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sr_20 = train_data_20.build_full_trainset()\n",
    "val_sr_before_20 = val_data_20.build_full_trainset()\n",
    "val_sr_20 = val_sr_before_20.build_testset()\n",
    "test_sr_before_20 = test_data_20.build_full_trainset()\n",
    "test_sr_20 = test_sr_before_20.build_testset()\n",
    "\n",
    "train_sr_50 = train_data_50.build_full_trainset()\n",
    "val_sr_before_50 = val_data_20.build_full_trainset()\n",
    "val_sr_50 = val_sr_before_20.build_testset()\n",
    "test_sr_before_50 = test_data_50.build_full_trainset()\n",
    "test_sr_50 = test_sr_before_50.build_testset()\n",
    "\n",
    "train_sr_100 = train_data_100.build_full_trainset()\n",
    "val_sr_before_100 = val_data_100.build_full_trainset()\n",
    "val_sr_100 = val_sr_before_100.build_testset()\n",
    "test_sr_before_100 = test_data_100.build_full_trainset()\n",
    "test_sr_100 = test_sr_before_100.build_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_tune = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = [5, 7, 10]  # the number of iteration of the SGD procedure\n",
    "lr_all = [0.002, 0.003, 0.005] # the learning rate for all parameters\n",
    "reg_all =  [0.4, 0.5, 0.6] # the regularization term for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4159\n",
      "RMSE: 1.4173\n",
      "RMSE: 1.4174\n",
      "RMSE: 1.4012\n",
      "RMSE: 1.4016\n",
      "RMSE: 1.4032\n",
      "RMSE: 1.3798\n",
      "RMSE: 1.3807\n",
      "RMSE: 1.3821\n",
      "RMSE: 1.4042\n",
      "RMSE: 1.4047\n",
      "RMSE: 1.4058\n",
      "RMSE: 1.3873\n",
      "RMSE: 1.3885\n",
      "RMSE: 1.3900\n",
      "RMSE: 1.3650\n",
      "RMSE: 1.3665\n",
      "RMSE: 1.3672\n",
      "RMSE: 1.3895\n",
      "RMSE: 1.3903\n",
      "RMSE: 1.3918\n",
      "RMSE: 1.3714\n",
      "RMSE: 1.3735\n",
      "RMSE: 1.3747\n",
      "RMSE: 1.3497\n",
      "RMSE: 1.3508\n",
      "RMSE: 1.3527\n"
     ]
    }
   ],
   "source": [
    "for n in n_epochs:\n",
    "    for l in lr_all:\n",
    "        for r in reg_all:\n",
    "            algo = SVD(n_epochs = n, lr_all = l, reg_all = r)\n",
    "            algo.fit(train_sr_20)\n",
    "            predictions = algo.test(val_sr_20)\n",
    "            RMSE_tune[n,l,r] = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(5, 0.002, 0.4): 1.415933676012248,\n",
       " (5, 0.002, 0.5): 1.4173365973852405,\n",
       " (5, 0.002, 0.6): 1.417370198820145,\n",
       " (5, 0.003, 0.4): 1.4011758726577428,\n",
       " (5, 0.003, 0.5): 1.4016120313912452,\n",
       " (5, 0.003, 0.6): 1.4031618552800484,\n",
       " (5, 0.005, 0.4): 1.3798484285012471,\n",
       " (5, 0.005, 0.5): 1.3806636393067282,\n",
       " (5, 0.005, 0.6): 1.3821350022120953,\n",
       " (7, 0.002, 0.4): 1.4042009585091328,\n",
       " (7, 0.002, 0.5): 1.4047180140602467,\n",
       " (7, 0.002, 0.6): 1.405780300835095,\n",
       " (7, 0.003, 0.4): 1.3872838266494933,\n",
       " (7, 0.003, 0.5): 1.3885483890855905,\n",
       " (7, 0.003, 0.6): 1.3900448377490269,\n",
       " (7, 0.005, 0.4): 1.3649676001324043,\n",
       " (7, 0.005, 0.5): 1.3665228974538999,\n",
       " (7, 0.005, 0.6): 1.3671758260897764,\n",
       " (10, 0.002, 0.4): 1.3894940834924787,\n",
       " (10, 0.002, 0.5): 1.3903377177304177,\n",
       " (10, 0.002, 0.6): 1.3917867965697626,\n",
       " (10, 0.003, 0.4): 1.3714281812046991,\n",
       " (10, 0.003, 0.5): 1.3734950396997587,\n",
       " (10, 0.003, 0.6): 1.3746840315086657,\n",
       " (10, 0.005, 0.4): 1.3497144262970868,\n",
       " (10, 0.005, 0.5): 1.3507892432826967,\n",
       " (10, 0.005, 0.6): 1.3527005637847467}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE_tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the best is when n_epochs = 10, lr_all = 0.005, reg_all = 0.4,\n",
    "# and the RMSE score is 1.3497\n",
    "# train and test on the optimal parameter\n",
    "algo = SVD(n_epochs = 10, lr_all = 0.005, reg_all = 0.4, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1b810dc748>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "algo.fit(train_sr_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 23.70190405845642 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3985173359167238"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_20 = algo.test(test_sr_20)\n",
    "accuracy.rmse(predictions_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1b810dc748>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "algo.fit(train_sr_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 58.85739994049072 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3794980281447085"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_50 = algo.test(test_sr_50)\n",
    "accuracy.rmse(predictions_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1b810dc748>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "algo.fit(train_sr_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 122.12709093093872 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4097218976062662"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_100 = algo.test(test_sr_100)\n",
    "accuracy.rmse(predictions_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train_final_20 = ratings_train_20.append(ratings_val_20)\n",
    "ratings_train_final_50 = ratings_train_50.append(ratings_val_50)\n",
    "ratings_train_final_100 = ratings_train_100.append(ratings_val_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_entire_df_20 = ratings_train_20.append(ratings_val_20).append(ratings_holdout_20)\n",
    "ratings_entire_df_50 = ratings_train_50.append(ratings_val_50).append(ratings_holdout_50)\n",
    "ratings_entire_df_100 = ratings_train_100.append(ratings_val_100).append(ratings_holdout_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_city_businesses_20 = ratings_entire_df_20[['city','business_id']].drop_duplicates()\n",
    "unique_cities_20 = unique_city_businesses_20.groupby('city').count()['business_id']\n",
    "unique_cities_20 = unique_cities_20[unique_cities > 100]\n",
    "out_20 = pd.DataFrame()\n",
    "for city in unique_cities_20.index:\n",
    "    tmp = ratings_holdout_20[(ratings_holdout_20['city'] ==city) &\n",
    "                              (ratings_holdout_20['rating'] >ratings_holdout_20['average_stars'])]\n",
    "    if len(tmp['user_id'].unique())>4:\n",
    "        \n",
    "        ###this weird sampling technique is to ensure we dont' sample the same user twice in a same city\n",
    "        five_users = np.random.choice(tmp['user_id'].unique(),5, replace = False)\n",
    "        row = tmp[tmp['user_id'].isin(five_users)].groupby('user_id', group_keys=False).apply(lambda df: df.sample(1))\n",
    "        out_20 = out_20.append(row)\n",
    "        \n",
    "unique_city_businesses_50 = ratings_entire_df_50[['city','business_id']].drop_duplicates()\n",
    "unique_cities_50 = unique_city_businesses_50.groupby('city').count()['business_id']\n",
    "unique_cities_50 = unique_cities_50[unique_cities > 100]\n",
    "out_50 = pd.DataFrame()\n",
    "for city in unique_cities.index:\n",
    "    tmp = ratings_holdout_50[(ratings_holdout_50['city'] ==city) &\n",
    "                              (ratings_holdout_50['rating'] >ratings_holdout_50['average_stars'])]\n",
    "    if len(tmp['user_id'].unique())>4:\n",
    "        \n",
    "        ###this weird sampling technique is to ensure we dont' sample the same user twice in a same city\n",
    "        five_users = np.random.choice(tmp['user_id'].unique(),5, replace = False)\n",
    "        row = tmp[tmp['user_id'].isin(five_users)].groupby('user_id', group_keys=False).apply(lambda df: df.sample(1))\n",
    "        out_50 = out_50.append(row)\n",
    "        \n",
    "unique_city_businesses_100 = ratings_entire_df_100[['city','business_id']].drop_duplicates()\n",
    "unique_cities_100 = unique_city_businesses_100.groupby('city').count()['business_id']\n",
    "unique_cities_100 = unique_cities_100[unique_cities > 100]\n",
    "out_100 = pd.DataFrame()\n",
    "for city in unique_cities.index:\n",
    "    tmp = ratings_holdout_100[(ratings_holdout_100['city'] ==city) &\n",
    "                              (ratings_holdout_100['rating'] >ratings_holdout_100['average_stars'])]\n",
    "    if len(tmp['user_id'].unique())>4:\n",
    "        \n",
    "        ###this weird sampling technique is to ensure we dont' sample the same user twice in a same city\n",
    "        five_users = np.random.choice(tmp['user_id'].unique(),5, replace = False)\n",
    "        row = tmp[tmp['user_id'].isin(five_users)].groupby('user_id', group_keys=False).apply(lambda df: df.sample(1))\n",
    "        out_100 = out_100.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df_20 = out_20[['user_id','city','state']]\n",
    "predict_df_20 = predict_df_20.merge(unique_city_businesses_20, on = 'city')\n",
    "predict_df_20['predictions'] = 25\n",
    "\n",
    "predict_df_50 = out_50[['user_id','city','state']]\n",
    "predict_df_50 = predict_df_50.merge(unique_city_businesses_50, on = 'city')\n",
    "predict_df_50['predictions'] = 25\n",
    "\n",
    "predict_df_100 = out_50[['user_id','city','state']]\n",
    "predict_df_100 = predict_df_100.merge(unique_city_businesses_100, on = 'city')\n",
    "predict_df_100['predictions'] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_20 = Dataset.load_from_df(predict_df_20[['user_id','business_id','predictions']], reader)\n",
    "eval_50 = Dataset.load_from_df(predict_df_50[['user_id','business_id','predictions']], reader)\n",
    "eval_100 = Dataset.load_from_df(predict_df_100[['user_id','business_id','predictions']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_before_20 = eval_20.build_full_trainset()\n",
    "eval_sr_20 = eval_before_20.build_testset()\n",
    "algo.fit(train_sr_20)\n",
    "eval_pred_20 = algo.test(eval_sr_20)\n",
    "#accuracy.rmse(predictions_20)\n",
    "baseline_20 = pd.DataFrame(eval_20, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "predict_df_20['predictions'] = baseline_20.pred_rating\n",
    "\n",
    "eval_before_50 = eval_50.build_full_trainset()\n",
    "eval_sr_50 = eval_before_50.build_testset()\n",
    "algo.fit(train_sr_50)\n",
    "eval_pred_50 = algo.test(eval_sr_50)\n",
    "#accuracy.rmse(predictions_50)\n",
    "baseline_50 = pd.DataFrame(eval_50, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "\n",
    "eval_before_100 = eval_100.build_full_trainset()\n",
    "eval_sr_100 = eval_before_100.build_testset()\n",
    "algo.fit(train_sr_100)\n",
    "eval_pred_100 = algo.test(eval_sr_100)\n",
    "#accuracy.rmse(predictions_100)\n",
    "baseline_100 = pd.DataFrame(eval_100, columns = ['userId','itemId','rating','pred_rating','x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(predict_df, validation_subsample, ratings_train_final):\n",
    "    top_10_recs = predict_df.groupby(['user_id','city'])['predictions'].nlargest(10).reset_index()\n",
    "    out = validation_subsample\n",
    "    cnt =0\n",
    "    serendipity = 0\n",
    "    \n",
    "    \n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        top_10 = predict_df.loc[top_10_recs[top_10_recs['user_id'] == row_values['user_id']].level_2]['business_id']\n",
    "        ###In top 10\n",
    "        if row_values['business_id'] in top_10.values:\n",
    "            cnt+=1\n",
    "        user_history = ratings_train_final[ratings_train_final['user_id'] == row_values['user_id']]    \n",
    "        been_there = [i for i in top_10.values if i in  user_history.business_id.values]\n",
    "        serendipity += 1-len(been_there)/10\n",
    "    \n",
    "    top_10 = cnt/len(out)\n",
    "    serendipity = serendipity/len(out)\n",
    "    \n",
    "    predict_df = predict_df.reset_index()\n",
    "    \n",
    "    analysis_df = predict_df.merge(top_10_recs, left_on = ['user_id','city','index'], right_on = ['user_id','city','level_2'])\n",
    "    \n",
    "    coverage = (analysis_df.groupby('city')['business_id'].nunique()/50).values.mean()\n",
    "    \n",
    "    predict_df['rankings']=predict_df.groupby(['city','user_id'])['predictions'].rank(\"first\",ascending = False)\n",
    "    running_rankings =0\n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        user_recs = predict_df[(predict_df['user_id']==row_values['user_id'])\n",
    "                            &(predict_df['city']==row_values['city'])\n",
    "                             & (predict_df['business_id']==row_values['business_id'])\n",
    "                              ]\n",
    "        assert len(user_recs)==1\n",
    "        running_rankings += user_recs['rankings'].sum()\n",
    "\n",
    "    avg_rank = running_rankings / len(out)\n",
    "    print(top_10, coverage, serendipity, avg_rank)\n",
    "    \n",
    "    return top_10, coverage, serendipity, avg_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13095238095238096 0.48214285714285715 0.9723809523809518 513.2166666666667\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out_20, ratings_train_final_20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_50, out_50, ratings_train_final_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_100, out_100, ratings_train_final_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal X_train_20, 50, 100\n",
    "X_train_20 = ratings_train_20.iloc[:, 0:3]\n",
    "X_train_20.columns = ['UserId','ItemId','Rating']\n",
    "\n",
    "X_train_50 = ratings_train_50.iloc[:, 0:3]\n",
    "X_train_50.columns = ['UserId','ItemId','Rating']\n",
    "\n",
    "X_train_100 = ratings_train_100.iloc[:, 0:3]\n",
    "X_train_100.columns = ['UserId','ItemId','Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal X_test_20, 50, 100\n",
    "X_test_20 = ratings_holdout_20.iloc[:, 0:3]\n",
    "X_test_20.columns = ['UserId','ItemId','Rating']\n",
    "\n",
    "X_test_50 = ratings_holdout_50.iloc[:, 0:3]\n",
    "X_test_50.columns = ['UserId','ItemId','Rating']\n",
    "\n",
    "X_test_100 = ratings_holdout_100.iloc[:, 0:3]\n",
    "X_test_100.columns = ['UserId','ItemId','Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal X_val_20\n",
    "X_val_20 = ratings_val_20.iloc[:,0:3]\n",
    "X_val_20.columns = ['UserId','ItemId','Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. State Average Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state average rating\n",
    "state_avg_20 = pd.DataFrame(ratings_train_20.groupby(\"state\").rating.mean())\n",
    "state_avg_20.columns = ['state_avg']\n",
    "train_state_avg_20 = ratings_train_20.merge(state_avg_20, on = \"state\")\n",
    "\n",
    "state_avg_50 = pd.DataFrame(ratings_train_50.groupby(\"state\").rating.mean())\n",
    "state_avg_50.columns = ['state_avg']\n",
    "train_state_avg_50 = ratings_train_50.merge(state_avg_50, on = \"state\")\n",
    "\n",
    "state_avg_100 = pd.DataFrame(ratings_train_100.groupby(\"state\").rating.mean())\n",
    "state_avg_100.columns = ['state_avg']\n",
    "train_state_avg_100 = ratings_train_100.merge(state_avg_100, on = \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item additional info: state average\n",
    "item_avg_20 = train_state_avg_20.loc[:,['business_id','state_avg']]\n",
    "item_avg_20.columns = ['ItemId','state_avg']\n",
    "\n",
    "item_avg_50 = train_state_avg_50.loc[:,['business_id','state_avg']]\n",
    "item_avg_50.columns = ['ItemId','state_avg']\n",
    "\n",
    "item_avg_100 = train_state_avg_100.loc[:,['business_id','state_avg']]\n",
    "item_avg_100.columns = ['ItemId','state_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_main = [0.5, 5.0, 10.0] # weight assign to the MRSE in factorization of the ratings matrix\n",
    "w_item = [0.5, 5.0, 10.0][::-1] # weight assign to the MRSE in factorization of the item attributes matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning\n",
    "for m in w_main:\n",
    "    for i in w_item:\n",
    "        model = CMF(w_main = m, w_item = i, random_seed = 1)\n",
    "        model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(item_state_avg_20))\n",
    "        prediction = model_1_tune.predict(X_val_20.user_id, X_val_20.business_id)\n",
    "        X_val_20['pred_rating'] = prediction\n",
    "        tune[m,i] = np.sqrt(np.mean((X_val_20.pred_rating - X_val_20.rating)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best param\n",
    "model = CMF(w_main = m, w_item = i, random_seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20%\n",
    "start_time = time.time()\n",
    "model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(item_state_avg_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_prediction_20 = model.predict(X_test_20.user_id, X_test_20.business_id)\n",
    "X_test_20['pred_rating'] = prediction\n",
    "print('RMSE (with 20% data): ', np.sqrt(np.mean((X_test_20.pred_rating - X_test_20.rating)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50%\n",
    "start_time = time.time()\n",
    "model.fit(ratings = deepcopy(X_train_50), item_info = deepcopy(item_state_avg_50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_prediction_50 = model_1_tune.predict(X_test_50.user_id, X_test_50.business_id)\n",
    "X_test_50['pred_rating'] = prediction\n",
    "print('RMSE (with 50% data): ', np.sqrt(np.mean((X_test_50.pred_rating - X_test_50.rating)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%\n",
    "start_time = time.time()\n",
    "model.fit(ratings = deepcopy(X_train_100), item_info = deepcopy(item_state_avg_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_prediction_100 = model.predict(X_test_100.user_id, X_test_100.business_id)\n",
    "X_test_100['pred_rating'] = prediction\n",
    "print('RMSE (with 100% data): ', np.sqrt(np.mean((X_test_100.pred_rating - X_test_100.rating)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state average rating\n",
    "state_avg_20 = pd.DataFrame(ratings_train_20.groupby(\"state\").rating.mean())\n",
    "state_avg_20.columns = ['state_avg']\n",
    "ratings_train_20 = ratings_train_20.merge(state_avg, on = \"state\")\n",
    "\n",
    "state_avg_50 = pd.DataFrame(ratings_train_50.groupby(\"state\").rating.mean())\n",
    "state_avg_50.columns = ['state_avg']\n",
    "ratings_train_50 = ratings_train_50.merge(state_avg, on = \"state\")\n",
    "\n",
    "state_avg_100 = pd.DataFrame(ratings_train_100.groupby(\"state\").rating.mean())\n",
    "state_avg_100.columns = ['state_avg']\n",
    "ratings_train_100 = ratings_train_100.merge(state_avg, on = \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make X_train\n",
    "X_train_20 = ratings_train_20.iloc[:, 0:3]\n",
    "X_train_20.columns = ['UserId','ItemId','Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item additional info: state average\n",
    "item_state_avg_20 = ratings_train.loc[:,['business_id','state_avg']]\n",
    "item_state_avg_20.columns = ['ItemId','state_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "X_val_20 = ratings_val_20.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_main = [0.5, 5.0, 10.0] # weight assign to the MRSE in factorization of the ratings matrix\n",
    "w_item = [0.5, 5.0, 10.0][::-1] # weight assign to the MRSE in factorization of the item attributes matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning\n",
    "for m in w_main:\n",
    "    for i in w_item:\n",
    "        model = CMF(w_main = m, w_item = i, random_seed = 1)\n",
    "        model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(item_state_avg_20))\n",
    "        prediction = model_1_tune.predict(X_val_20.user_id, X_val_20.business_id)\n",
    "        X_val_20['pred_rating'] = prediction\n",
    "        tune_1[m,i] = np.sqrt(np.mean((X_val_20.pred_rating - X_val_20.rating)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.fit(ratings = deepcopy(X_train), item_info = deepcopy(Y_train),\\\n",
    "            cols_bin_item=[cl for cl in Y_train.columns if cl != 'ItemId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. User Average Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state average rating\n",
    "state_avg_20 = pd.DataFrame(ratings_train_20.groupby(\"state\").rating.mean())\n",
    "state_avg_20.columns = ['state_avg']\n",
    "ratings_train_20 = ratings_train_20.merge(state_avg, on = \"state\")\n",
    "\n",
    "state_avg_50 = pd.DataFrame(ratings_train_50.groupby(\"state\").rating.mean())\n",
    "state_avg_50.columns = ['state_avg']\n",
    "ratings_train_50 = ratings_train_50.merge(state_avg, on = \"state\")\n",
    "\n",
    "state_avg_100 = pd.DataFrame(ratings_train_100.groupby(\"state\").rating.mean())\n",
    "state_avg_100.columns = ['state_avg']\n",
    "ratings_train_100 = ratings_train_100.merge(state_avg, on = \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make X_train\n",
    "X_train_20 = ratings_train_20.iloc[:, 0:3]\n",
    "X_train_20.columns = ['UserId','ItemId','Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item additional info: state average\n",
    "item_state_avg_20 = ratings_train.loc[:,['business_id','state_avg']]\n",
    "item_state_avg_20.columns = ['ItemId','state_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "X_val_20 = ratings_val_20.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_main = [0.5, 5.0, 10.0] # weight assign to the MRSE in factorization of the ratings matrix\n",
    "w_item = [0.5, 5.0, 10.0][::-1] # weight assign to the MRSE in factorization of the item attributes matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning\n",
    "for m in w_main:\n",
    "    for i in w_item:\n",
    "        model = CMF(w_main = m, w_item = i, random_seed = 1)\n",
    "        model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(item_state_avg_20))\n",
    "        prediction = model_1_tune.predict(X_val_20.user_id, X_val_20.business_id)\n",
    "        X_val_20['pred_rating'] = prediction\n",
    "        tune_1[m,i] = np.sqrt(np.mean((X_val_20.pred_rating - X_val_20.rating)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% data\n",
    "user_id_unique = reviews.user_id.unique()\n",
    "user_id_sample = pd.DataFrame(user_id_unique, columns=['unique_user_id'])\n",
    "ratings_sample = reviews.merge(user_id_sample, left_on = 'user_id', right_on = 'unique_user_id').drop(['unique_user_id'], axis = 1)\n",
    "print(ratings_sample.head())\n",
    "print(ratings_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
